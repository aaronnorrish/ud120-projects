### Lesson 12.17
Should have:
```
regression.fit(features, labels)
```

### Lesson 12.18
The line should be:
```
regression.predict([[2,4]])
```

### Lesson 12.19
To obtain the coefficients:
```
regression.coef_
```

### Lesson 12.23
You would expect that the accuracy on a test set for an overfit decision tree would be low.

### Lesson 12.24
Conversely, it would have a high accuracy for the training set.

### Lesson 12.25
There are 150 training points.

### Lesson 12.26
The accuracy of the decision tree is 0.947667804323.

### Lesson 12.27
The importance of the most important feature is 0.764705882353. The number of the feature is 33614. It is the only feature that has an importance greater than 0.2.

### Lesson 12.28
The most important word is "sshacklensf".

### Lesson 12.29
The next most important word is "cgermannsf".

### Lesson 12.30
After removing "cgermannsf" the next most important word is "houectect". It is unclear whether this is a signature.

### Lesson 12.31
With the two signature words now removed, the accuracy of the decision tree is 0.816837315131.
